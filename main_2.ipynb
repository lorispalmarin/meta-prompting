{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prompting for Hybrid Fairy-Tales\n",
    "Comparing Prompt Engineering Strategies Across LLMs"
   ],
   "id": "16b7db0f7cb1acc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup\n",
    "\n",
    "### Import Libraries"
   ],
   "id": "b9ffbca7ecacbe70"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "! pip install -r requirements.txt",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:46:22.820002Z",
     "start_time": "2025-07-21T13:46:22.788702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import json\n",
    "import sys\n",
    "from models.models import *\n",
    "from llama_cpp import Llama\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "together_key = os.getenv(\"TOGETHERAI_API_KEY\")"
   ],
   "id": "d5757f9cabeb7491",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "### Load Prompt data"
   ],
   "id": "162716d13538f98f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I created a JSON containing the prompts",
   "id": "9e418db8b12744af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:20:27.443432Z",
     "start_time": "2025-07-21T10:20:27.427965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('prompts/prompts.json', 'r') as f:\n",
    "    meta_prompts = json.load(f)\n",
    "\n",
    "meta_prompts"
   ],
   "id": "fa4db14b8bbaad0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Technique': 'Zero-Shot',\n",
       "  'System': 'You are an AI language model tasked with writing short and imaginative fairy tales. In each case, you will be asked to write a fairy tale that blends two contrasting narrative genres. Do not assume any prior context or style. Simply respond with a complete and coherent fairy tale based solely on the genres provided. Use clear language and avoid adding your own framing or interpretations.'},\n",
       " {'Technique': 'Role Prompting',\n",
       "  'System': 'You are a professional fairy tale writer known for your mastery in combining unconventional literary styles. Your task is to write a short fairy tale that skillfully merges two contrasting genres, as specified by the user. Your storytelling should demonstrate creative discipline, blending the genres in a balanced way while remaining entertaining and coherent. Maintain a polished tone and ensure that the story reflects the unique narrative features of both genres.'},\n",
       " {'Technique': 'Few-Shot',\n",
       "  'System': 'Below are two examples of how to write creative fairy tales combining different genres:\\n\\nExample 1: A cyberpunk-fantasy tale about a magical hacker who reprograms dreams in a neon-lit forest.\\nExample 2: A noir-folklore story in which a cursed detective solves ghost crimes in a forgotten village.\\n\\nNow, using the same creative format and narrative balance, write a new fairy tale based on the two genres provided by the user. Blend the styles meaningfully and maintain the tone of a cohesive and well-written story.'},\n",
       " {'Technique': 'Style Prompting',\n",
       "  'System': 'Write the fairy tale using rich, evocative language and a strong narrative voice. Your task is to weave a story that merges two contrasting genres given by the user — such as horror and comedy, or myth and sci-fi — into a seamless narrative. The style should reflect literary flair and poetic rhythm while ensuring that the fusion of genres feels natural and deliberate. Make the story stylistically distinct, imaginative, and immersive.'},\n",
       " {'Technique': 'Emotion + Zero-Shot',\n",
       "  'System': 'Respond with a fairy tale that evokes emotion, mystery, and wonder. Without assuming any specific structure or role, generate a story based solely on the two contrasting genres provided. Let the tone shift gracefully between light and dark, rational and magical, absurd and solemn. Use emotionally rich language that gives life to contradictions and inspires a sense of narrative tension and beauty.'},\n",
       " {'Technique': 'Emotion + Role Prompting',\n",
       "  'System': 'You are a dream-weaver, a generative storyteller whose job is to create tales that blend the impossible. Given two contrasting genres by the user, you must invent a fairy tale that allows these styles to dance together. Let your words be filled with imagination, emotion, and enchantment. Craft stories that feel like contradictions resolved — where fear embraces laughter, or logic meets myth. Let your narrative voice express both structure and surprise.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 2. Prompting plan"
   ],
   "id": "870d2d758d6a2d84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a first view, we can convert the meta-prompts into a Pandas DataFrame and display them.",
   "id": "66ba0b66dbe1181"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:46:49.707875Z",
     "start_time": "2025-07-21T13:46:49.663945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompts_df = pd.DataFrame(system_prompts)\n",
    "system_prompts_df\n"
   ],
   "id": "875167fd31510945",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  Technique                                             System\n",
       "0                 Zero-Shot  You are an AI language model tasked with writi...\n",
       "1            Role Prompting  You are a professional fairy tale writer known...\n",
       "2                  Few-Shot  Below are two examples of how to write creativ...\n",
       "3           Style Prompting  Write the fairy tale using rich, evocative lan...\n",
       "4       Emotion + Zero-Shot  Respond with a fairy tale that evokes emotion,...\n",
       "5  Emotion + Role Prompting  You are a dream-weaver, a generative storytell..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Technique</th>\n",
       "      <th>System</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zero-Shot</td>\n",
       "      <td>You are an AI language model tasked with writi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Role Prompting</td>\n",
       "      <td>You are a professional fairy tale writer known...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Few-Shot</td>\n",
       "      <td>Below are two examples of how to write creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Style Prompting</td>\n",
       "      <td>Write the fairy tale using rich, evocative lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emotion + Zero-Shot</td>\n",
       "      <td>Respond with a fairy tale that evokes emotion,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Emotion + Role Prompting</td>\n",
       "      <td>You are a dream-weaver, a generative storytell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The two genres that will be blended are: **crime** +  **fantasy**",
   "id": "9f7f682d41a9b306"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:20:33.055386Z",
     "start_time": "2025-07-21T10:20:33.049449Z"
    }
   },
   "cell_type": "code",
   "source": "user_prompt = \"The fairy tale will mix the two following genres: crime + fantasy\"",
   "id": "429863bcabe28ddf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 3. Generation\n",
    "\n",
    "In this section, we generate hybrid fairy tales using six different prompting strategies across three large language models (LLMs).\n",
    "Each story is produced in response to a fixed user instruction — asking the model to generate a fairy tale blending two contrasting narrative genres — and guided by a system prompt that embodies one of the six prompting techniques defined earlier.\n",
    "\n",
    "We use the following models:\n",
    "\n",
    "- **GPT-4** (OpenAI): a strong commercial model, used as a qualitative reference for generation quality.\n",
    "  ⚠️ *This model does not provide internal explainability tools like logprobs.*\n",
    "\n",
    "- **LLaMA 3.1–8B** (via TogetherAI API): an open-source model.\n",
    "  ✅ *Provides token-level logprobs and supports direct XAI-style analysis.*\n",
    "\n",
    "- **Mistral 7B** (via TogetherAI API): still an open-source model.\n",
    "\n",
    "\n",
    "Each model receives the same pair of inputs:\n",
    "1. A **system prompt** representing one of the six prompting techniques.\n",
    "2. A **user prompt** that specifies the genres to be blended (e.g., \"Please write a fairy tale that combines horror and comedy.\").\n",
    "\n",
    "This setup allows us to:\n",
    "- Observe how different prompting strategies shape the story generation process,\n",
    "- Compare how each model interprets the same input,\n",
    "- Apply explainability tools (logprobs) to open models to measure which parts of the prompt influence the output most.\n",
    "\n",
    "The following subsections are dedicated to the generation process for each model individually.\n",
    "\n",
    "#### Generation: GPT-4 (Reference)"
   ],
   "id": "e5886b8a0f30a6b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T13:52:34.678731Z",
     "start_time": "2025-07-21T13:51:18.445634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompts = system_prompts\n",
    "gpt4_outputs = []\n",
    "\n",
    "# Loop on each prompt\n",
    "for entry in system_prompts:\n",
    "    technique = entry[\"Technique\"]\n",
    "    system_prompt = entry[\"System\"]\n",
    "\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    output = query_gpt4(full_prompt)\n",
    "\n",
    "    gpt4_outputs.append({\n",
    "        \"Technique\": technique,\n",
    "        \"GeneratedText\": output\n",
    "    })\n",
    "\n",
    "with open(\"outputs/gpt-4o_tales.json\", \"w\") as f:\n",
    "    json.dump(gpt4_outputs, f, indent=2)"
   ],
   "id": "fd63139c4ccf3e2e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:22:06.539693Z",
     "start_time": "2025-07-21T10:22:06.534298Z"
    }
   },
   "cell_type": "code",
   "source": "gpt4_outputs",
   "id": "c79e29543f4ad6eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Technique': 'Zero-Shot',\n",
       "  'GeneratedText': \"Once upon a time, in the mystical land of Ailuria, where fairies twinkled under the crescent moon and unicorns graced the verdant meadows, the harmony of this enchanted realm was suddenly disrupted. The precious Moonstone, the divine artifact which preserved Ailuria's magic, had been stolen. The disappearance of the Moonstone meant the depletion of magic, the doom of Ailuria, where the line between crime and fantasy began to blur.\\n\\nThe investigation was led by Detective Owliver, a wise owl with a silver badge pinned to his feathery chest and a pair of spectacles perched on his beak. Even though Owliver was knowledgeable about crimes, he was still a rookie when it came to magic and fantasy. However, the urgency of the situation led him to take the mission, with a shimmering wand in his wing.\\n\\nHis first suspect was the sly fox, Reynard, known around Ailuria for his tricks and deceit. Owliver flew to Reynard's den, which was filled with pilfered trinkets and shiny baubles. But there was no sign of the Moonstone. Reynard, though shady, was innocent.\\n\\nThe detective did not lose hope. He then journeyed to the azure creek, home of the mermaid, Isolde, famous for her enchanting voice and her love for jewels. Her underwater cave, adorned with precious stones, was searched thoroughly. Yet, the Moonstone remained\"},\n",
       " {'Technique': 'Role Prompting',\n",
       "  'GeneratedText': \"Title: The Mystery of the Enchanted Chalice\\n\\nOnce upon a time, in the grand Kingdom of Emrys, a crime of extraordinary proportions happened. The Enchanted Chalice - a goblet imbued with magical properties - was stolen from the heavily guarded royal treasury. Designed to bring bountiful harvests and abundant prosperity, the chalice was the pride of Emrys.\\n\\nEmrys was not a conventional kingdom. It was a land where dragons soared across the sky, unicorns grazed in majestic meadows, and fairies flickered through the evergreen forests. Inhabitants possessed magical abilities, and the norms of the ordinary world were upended here.\\n\\nThe theft of the precious chalice sent shockwaves through the kingdom. It was a crime that dared to challenge the crowning glory of Emrys. As the tranquillity of the kingdom transformed into panic, the wise King Eldrid summoned Rafe Hemlock, the realm's most brilliant detective.\\n\\nRafe was a half-elf, half-human detective with a trench coat and a charismatic charm. He was known for his unorthodox methods and uncanny intuition. Rafe's unique gifts stemmed from his mixed heritage - from his elf mother, he inherited the ability to commune with natural elements, and from his human father, the knack for deduction. \\n\\nWith his magical pocket watch - a device that could record and replay any event - Rafe started investigating the pinpointed time of the theft. He observed the\"},\n",
       " {'Technique': 'Few-Shot',\n",
       "  'GeneratedText': 'A crime-fantasy tale about a stealthy thief who steals magical artifacts from the underworld, navigating through a city hidden beneath the human realm.'},\n",
       " {'Technique': 'Style Prompting',\n",
       "  'GeneratedText': \"Once upon a time, in a city bathed in spectral moonlight and shrouded in the obsidian blanket of night, there existed a world within a world. A realm where the ethereal collided with the tangible, blending matter and myth. It was an era of magic and modernity, where the supernatural spliced seamlessly with the seedy underbelly of crime. Welcome, dear reader, to the neon-lit labyrinth of Elysium City.\\n\\nOur tale meanders around a noir-clad enigma, Artemis Blackthorn, a detective of the metaphysical and mundane. With a swift mind as sharp as a Basilisk's fang and eyes as piercing as a dragon’s flame, he was an amalgamation of Sherlock Holmes and Merlin, stalking his prey in the shadows of both worlds.\\n\\nAn incubus of intrigue descended upon Elysium City. The Crown of the Seven Seas, a relic of pulsating magic and untold power, vanished mysteriously from the royal treasury. Its disappearance sent ripples across the variegated realms, stirring up a tempest of chaos and fear.\\n\\nArtemis was summoned by the city's regents. His reputation was staked on this case, an intoxicating blend of crime and fantasy. Duty-strapped, armed with the intriguing incandescence of his magic and his instinctive analytical prowess, he ventured into the cryptic labyrinth of his dual-world.\\n\\nHis journey tangled him in the dance of seductive sirens\"},\n",
       " {'Technique': 'Emotion + Zero-Shot',\n",
       "  'GeneratedText': \"Once, in the city of Argent, where every cobblestone was silver, and the buildings shimmered like a well-polished mirror, there was a notorious thief of magic, who went by the name of Ebon Shadow. He was named after the most intimidating time of the day, the time of ebon darkness, when the sun had sunk below the horizon and the last tinges of twilight had been swallowed by the night. Ebon was as elusive as time itself, slipping past fingers like water and melting into shadows, unseen.\\n\\nEbon did not steal gold or jewels. No, his targets were far more peculiar. He was a robber of magic, a plunderer of spells, his eyes covetous for enchanted objects and bewitched creatures. A warlock's staff, a witch's familiar, a unicorn's horn, all found their way into Ebon's hidden stash over the years.\\n\\nThe High Council of Magi, the ruling authority of Argent, had, for years, been trying to apprehend this slippery criminal, but Ebon was always a step ahead, melting into the silver city's shadows before justice could find him. \\n\\nOne day, a grand announcement was made. The legendary Phoenix Gem, an artifact of ancient magic, was to be unveiled in the grand Argent Cathedral. The jewel was said to possess the power to resurrect the dead, or grant eternal life. Such was its allure that it drew spectators from all corners of the world.\\n\\nThe Mag\"},\n",
       " {'Technique': 'Emotion + Role Prompting',\n",
       "  'GeneratedText': 'Once upon a time, in the fantastical city of Silver Springs, where unicorns were cab drivers and fairies were firefighters, there occurred a series of peculiar and troubling events that left the inhabitants of the city in a state of unease. They called it \"The Scepter Heist,\" a crime that mixed the dark art of theft with the magic of enchantments.\\n\\nIn the heart of Silver Springs stood the majestic Tower of Enchantment, home to the city\\'s most powerful artifact, the Starlight Scepter. The scepter was rumored to hold the magic that kept the city alive and flourishing, the magic that made the wilting flowers bloom and the night skies glitter with an unmatched wonder that drew creatures from all dimensions. It was protected by enchantments so powerful, even the wiliest of goblins feared to enter its shimmering walls.\\n\\nHowever, one eerie night under the veil of a new moon, the Starlight Scepter vanished. The city woke up to the morose tolling of the tower bells and a sky without the usual dance of the fairies. The dread was palpable in Silver Springs as its residents mourned the loss of their enchanted protector.\\n\\nThe Silver Springs Police Department, consisting of wizards, elves, nymphs, and even a few dwarves, was flummoxed. They had never encountered a crime like this before. How could someone bypass the spells protecting the scepter and vanish without leaving a trace?\\n\\nInspector Glimmer,'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generation + XAI: LlaMA 3.1 - 8B",
   "id": "e1debe97984d5a8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T14:20:05.588652Z",
     "start_time": "2025-07-21T14:19:25.575410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Client Initialization\n",
    "client = Together()\n",
    "\n",
    "# Model Selection\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\n",
    "\n",
    "llama_outputs = []\n",
    "\n",
    "for entry in system_prompts:\n",
    "    technique = entry[\"Technique\"]\n",
    "    system_prompt = entry[\"System\"]\n",
    "\n",
    "    prompt = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 700,\n",
    "        \"temperature\": 0.8,\n",
    "        \"logprobs\": 5  # <- qui richiedi i logprobs\n",
    "    }\n",
    "\n",
    "# Generation\n",
    "    resp = client.chat.completions.create(**prompt)\n",
    "\n",
    "# Extract and save output\n",
    "    choice = resp.choices[0]\n",
    "    generated = choice.message.content\n",
    "    log = choice.logprobs  # contiene tokens, token_logprobs, top_logprobs\n",
    "\n",
    "    result = {\n",
    "        \"model\": model_name,\n",
    "        \"technique\": technique,\n",
    "        \"generated\": generated,\n",
    "        \"tokens\": log.tokens,\n",
    "        \"token_logprobs\": log.token_logprobs,\n",
    "        \"top_logprobs\": log.top_logprobs\n",
    "    }\n",
    "    llama_outputs.append(result)\n",
    "\n",
    "with open(\"outputs/together_llama_test.json\", \"w\") as f:\n",
    "    json.dump(llama_outputs, f, indent=2)"
   ],
   "id": "cc27ca6a740691c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### LM Tentative with ollama.cpp",
   "id": "caaa3a47ff48a251"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T15:24:24.189544Z",
     "start_time": "2025-07-17T15:13:35.420647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = Llama(\n",
    "    model_path=\"/Users/lorispalmarin/projects/llama.cpp/models/llama3.2_1B.gguf\",\n",
    "    logprobs=True,\n",
    "    n_ctx=512\n",
    ")\n",
    "\n",
    "llama_outputs = []\n",
    "\n",
    "for entry in system_prompts:\n",
    "    technique = entry[\"Technique\"]\n",
    "    system_prompt = entry[\"System\"]\n",
    "\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "    output = query_llama(full_prompt)\n",
    "\n",
    "    llama_outputs.append({\n",
    "        \"Technique\": technique,\n",
    "        \"GeneratedText\": output\n",
    "    })"
   ],
   "id": "f48e7e057e1679d0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M1) - 5455 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 146 tensors from /Users/lorispalmarin/projects/llama.cpp/models/llama3.2_1B.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = llama3.2_1B_hf\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 1.2B\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 16\n",
      "llama_model_loader: - kv   5:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 64\n",
      "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 64\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_sep_token bool             = false\n",
      "llama_model_loader: - type  f32:   33 tensors\n",
      "llama_model_loader: - type  f16:  113 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 2.30 GiB (16.00 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 16\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 512\n",
      "print_info: n_embd_v_gqa     = 512\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1B\n",
      "print_info: model params     = 1.24 B\n",
      "print_info: general.name     = llama3.2_1B_hf\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 146 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/17 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  2357.26 MiB\n",
      "..............................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x161c9bec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x13955a180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x139559700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x12ebabbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x12ebabe40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x139559380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x139558d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x139558a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x12ebac0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x12ebac300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x12ebac560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x139557310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x12ebac7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x12ebaca20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x12ebacc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x12ebacee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x12ebad140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x12ebad3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x12ff896f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x161ca4530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x12ff88d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x12ebad600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x12ff87f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ebad860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x12ebadac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x12ff87c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x12ff87880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x12ebadd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x12ebadf80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x12ff86110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x12ebae1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x12ebae440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x12ff85d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x12ff84250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12f168600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x139556f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x139556c10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ff83c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ff81bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x139555e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x139553910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x139553350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x139551ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x166b2fb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1395513b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13954ef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ff81830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x166b20e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x166b209a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x166b204c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13954dc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ff80cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13954d900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13954d580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ff80940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13a14ef30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x166b1ea60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13954d200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13954be80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x166b30750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x166b30010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x161c9de90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x13954b310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x166b59130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x161cab700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x13954af90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x166b59390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x12ff805c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x161cd58a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x166b595f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x12f168e70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x13954a420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x139549e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ff80240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x139549b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x166b59850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x12ff7fec0 | th_max =  384 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x1395491b0 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x161cd5b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x139548c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ff7fb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x1395488a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x139544a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x139544340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x139543e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x166b59ab0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12f169b40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13a14f190 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x161cd5d60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x166b59d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13a14f3f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13a14f650 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x139543980 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ff7f7c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x139542c00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ff7f440 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x139542620 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13a14f8b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x161cd5fc0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x166b59f70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13953d720 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x166b5a1d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x166b5a430 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12f169da0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13953eaa0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x166b5a690 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x166b5a8f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1394afd30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x166b5ab50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x161cd6220 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x166b5adb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x166b5b010 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x166b5b270 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12f16a000 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x161cd6480 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13954cd70 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x161cd66e0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ff7f0c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x161cd6940 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ff7ed40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ff7e9c0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ff7e640 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ff7e2c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x161cd6ba0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ff7d4f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x139574e00 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x161cd6e00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ff7cf30 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x139575060 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1394aff90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1394b01f0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13a14fb10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13a14fd70 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x166b5b4d0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x166b5b730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12f16a260 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x166b5b990 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13a14ffd0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13a150230 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ff7b770 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ff79c90 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ff79910 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13a150490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x166b5bbf0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ff79590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ff79210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x166b5be50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x166b5c0b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x166b5c310 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x166b5c570 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13a1506f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ff78e90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ff78b10 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13a150950 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ff77210 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ff76d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13a150bb0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ff76850 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ff76370 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ff75e90 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ff759b0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13a150e10 | th_max =  448 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ff743b0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13a151070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ff727f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ff60330 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ff73dd0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13a1512d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13a151530 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ff82810 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x166b5c7d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ff87410 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ff7de50 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x166b5ca30 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ff89a70 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x166b5cc90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x166b5cef0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x166b5d150 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1395752c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13a151790 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x166b5d3b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ffa2d00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x166b5d610 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ffa2f60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ffa31c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x139575520 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x139575780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x13a1519f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x1394b0450 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x1394b06b0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x161cd7060 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x166b5d870 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x166b5dad0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x12f16a4c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x166b5dd30 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x12f16a720 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x166b5df90 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x1395759e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x139575c40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x139575ea0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x139576100 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x139576360 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x1395765c0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x166b5e1f0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x12ffa3420 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x139576820 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x166b5e450 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x139576a80 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x166b5e6b0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x166b5e910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x139576ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x166b5eb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x139576f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x166b5edd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x166b5f030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ffa3680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x166b5f290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x12ffa38e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x166b5f4f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x166b5f750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x166b5f9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ffa3b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x166b5fc10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x166b5fe70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x12ffa3da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ffa4000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x166b600d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x166b60330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x166b60590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ffa4260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ffa44c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x166b607f0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x166b60a50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x166b60cb0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13a151c50 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13a151eb0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x1395771a0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x13a152110 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x139577400 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x13a152370 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13a1525d0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x139577660 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1395778c0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x139577b20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x166b60f10 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x166b61170 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x139577d80 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x139577fe0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x166b613d0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x166b61630 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x139578240 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1395784a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x139578700 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ffa4720 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x12ffa4980 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x166b61890 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x139578960 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x12ffa4be0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x139578bc0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ffa4e40 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ffa50a0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x166b61af0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x166b61d50 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x12ffa5300 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x12ffa5560 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ffa57c0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x12ffa5a20 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x139578e20 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x139579080 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1395792e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13a152830 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ffa5c80 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x139579540 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x12ffa5ee0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1395797a0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x12ffa6140 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x166b61fb0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x166b62210 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ffa63a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x139579a00 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ffa6600 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x12ffa6860 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x12ffa6ac0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x139579c60 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x12ffa6d20 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x139579ec0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x13957a120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x166b62470 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x13957a380 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x12ffa6f80 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x166b626d0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x166b62930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x13957a5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x166b62b90 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x166b62df0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x166b63050 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x13957a840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ffa71e0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ffa7440 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ffa76a0 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x166b632b0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ffa7900 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13957aaa0 | th_max =  896 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x12ffa7b60 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x13957ad00 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x13a152a90 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x13a152cf0 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x13957af60 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x13957b1c0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x12ffa7dc0 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x13957b420 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x12ffa8020 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x13957b680 | th_max =  768 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x13957b8e0 | th_max =  704 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x13957bb40 | th_max =  832 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x166b63510 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x166b63770 | th_max =  640 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13957bda0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x166b639d0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x166b63c30 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x166b63e90 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x166b640f0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x12ffa8280 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x166b64350 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x13957c000 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x161cd72c0 | th_max =  512 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x12ffa84e0 | th_max =  576 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x166b645b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x166b64810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13a152f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13957c260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13a1531b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ffa8740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13957c4c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13a153410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13957c720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13957c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13a153670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13a1538d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13957cbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x166b64a70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13957ce40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13a153b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13a153d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13957d0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13957d300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13957d560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13957d7c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13957da20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x13a153ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x166b64cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x166b64f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x166b65190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x13957dc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x166b653f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x13957dee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x13a154250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x13a1544b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x13957e140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x13a154710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x13a154970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x13957e3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x166b65650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13a154bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x166b658b0 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    16.00 MiB\n",
      "llama_kv_cache_unified: size =   16.00 MiB (   512 cells,  16 layers,  1 seqs), K (f16):    8.00 MiB, V (f16):    8.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   254.50 MiB\n",
      "llama_context: graph nodes  = 582\n",
      "llama_context: graph splits = 226 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | DOTPROD = 1 | LLAMAFILE = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.eos_token_id': '128001', 'tokenizer.ggml.add_sep_token': 'false', 'general.type': 'model', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '64', 'general.quantization_version': '2', 'llama.embedding_length': '2048', 'llama.vocab_size': '128256', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.attention.value_length': '64', 'llama.attention.head_count': '32', 'llama.attention.key_length': '64', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'general.file_type': '1', 'llama.block_count': '16', 'general.size_label': '1.2B', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.feed_forward_length': '8192', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.name': 'llama3.2_1B_hf'}\n",
      "Using fallback chat format: llama-2\n",
      "ggml_metal_free: deallocating\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "ggml_metal_mem_pool_free: freeing memory pool, num heaps = 0 (total = 0)\n",
      "llama_perf_context_print:        load time =    5461.34 ms\n",
      "llama_perf_context_print: prompt eval time =    5460.59 ms /    86 tokens (   63.50 ms per token,    15.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =  264218.01 ms /    63 runs   ( 4193.94 ms per token,     0.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  269799.40 ms /   149 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5461.34 ms\n",
      "llama_perf_context_print: prompt eval time =    4670.40 ms /    89 tokens (   52.48 ms per token,    19.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =  270458.76 ms /    63 runs   ( 4293.00 ms per token,     0.23 tokens per second)\n",
      "llama_perf_context_print:       total time =  275231.56 ms /   152 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 119 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     28\u001B[39m system_prompt = entry[\u001B[33m\"\u001B[39m\u001B[33mSystem\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m     30\u001B[39m full_prompt = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msystem_prompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00muser_prompt\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m output = query_llama(full_prompt)\n\u001B[32m     33\u001B[39m llama_outputs.append({\n\u001B[32m     34\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mTechnique\u001B[39m\u001B[33m\"\u001B[39m: technique,\n\u001B[32m     35\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mGeneratedText\u001B[39m\u001B[33m\"\u001B[39m: output\n\u001B[32m     36\u001B[39m })\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mquery_llama\u001B[39m\u001B[34m(prompt, max_tokens, temperature, top_k, top_p)\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mquery_llama\u001B[39m(prompt, max_tokens=\u001B[32m64\u001B[39m, temperature=\u001B[32m0.8\u001B[39m, top_k=\u001B[32m50\u001B[39m, top_p=\u001B[32m0.95\u001B[39m):\n\u001B[32m     12\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[33;03m    Interroga LLaMA e restituisce solo il testo generato.\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     output = llm(\n\u001B[32m     17\u001B[39m         prompt,\n\u001B[32m     18\u001B[39m         max_tokens=max_tokens,\n\u001B[32m     19\u001B[39m         temperature=temperature,\n\u001B[32m     20\u001B[39m         echo=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m     21\u001B[39m         top_k=top_k,\n\u001B[32m     22\u001B[39m         top_p=top_p\n\u001B[32m     23\u001B[39m     )\n\u001B[32m     24\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[33m\"\u001B[39m\u001B[33mchoices\u001B[39m\u001B[33m\"\u001B[39m][\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/llama.py:1904\u001B[39m, in \u001B[36mLlama.__call__\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1840\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\n\u001B[32m   1841\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1842\u001B[39m     prompt: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1866\u001B[39m     logit_bias: Optional[Dict[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1867\u001B[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001B[32m   1868\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Generate text from a prompt.\u001B[39;00m\n\u001B[32m   1869\u001B[39m \n\u001B[32m   1870\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1902\u001B[39m \u001B[33;03m        Response object containing the generated text.\u001B[39;00m\n\u001B[32m   1903\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1904\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_completion(\n\u001B[32m   1905\u001B[39m         prompt=prompt,\n\u001B[32m   1906\u001B[39m         suffix=suffix,\n\u001B[32m   1907\u001B[39m         max_tokens=max_tokens,\n\u001B[32m   1908\u001B[39m         temperature=temperature,\n\u001B[32m   1909\u001B[39m         top_p=top_p,\n\u001B[32m   1910\u001B[39m         min_p=min_p,\n\u001B[32m   1911\u001B[39m         typical_p=typical_p,\n\u001B[32m   1912\u001B[39m         logprobs=logprobs,\n\u001B[32m   1913\u001B[39m         echo=echo,\n\u001B[32m   1914\u001B[39m         stop=stop,\n\u001B[32m   1915\u001B[39m         frequency_penalty=frequency_penalty,\n\u001B[32m   1916\u001B[39m         presence_penalty=presence_penalty,\n\u001B[32m   1917\u001B[39m         repeat_penalty=repeat_penalty,\n\u001B[32m   1918\u001B[39m         top_k=top_k,\n\u001B[32m   1919\u001B[39m         stream=stream,\n\u001B[32m   1920\u001B[39m         seed=seed,\n\u001B[32m   1921\u001B[39m         tfs_z=tfs_z,\n\u001B[32m   1922\u001B[39m         mirostat_mode=mirostat_mode,\n\u001B[32m   1923\u001B[39m         mirostat_tau=mirostat_tau,\n\u001B[32m   1924\u001B[39m         mirostat_eta=mirostat_eta,\n\u001B[32m   1925\u001B[39m         model=model,\n\u001B[32m   1926\u001B[39m         stopping_criteria=stopping_criteria,\n\u001B[32m   1927\u001B[39m         logits_processor=logits_processor,\n\u001B[32m   1928\u001B[39m         grammar=grammar,\n\u001B[32m   1929\u001B[39m         logit_bias=logit_bias,\n\u001B[32m   1930\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/llama.py:1837\u001B[39m, in \u001B[36mLlama.create_completion\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1835\u001B[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001B[32m   1836\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m chunks\n\u001B[32m-> \u001B[39m\u001B[32m1837\u001B[39m completion: Completion = \u001B[38;5;28mnext\u001B[39m(completion_or_chunks)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[32m   1838\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m completion\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/llama.py:1322\u001B[39m, in \u001B[36mLlama._create_completion\u001B[39m\u001B[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001B[39m\n\u001B[32m   1320\u001B[39m finish_reason = \u001B[33m\"\u001B[39m\u001B[33mlength\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1321\u001B[39m multibyte_fix = \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.generate(\n\u001B[32m   1323\u001B[39m     prompt_tokens,\n\u001B[32m   1324\u001B[39m     top_k=top_k,\n\u001B[32m   1325\u001B[39m     top_p=top_p,\n\u001B[32m   1326\u001B[39m     min_p=min_p,\n\u001B[32m   1327\u001B[39m     typical_p=typical_p,\n\u001B[32m   1328\u001B[39m     temp=temperature,\n\u001B[32m   1329\u001B[39m     tfs_z=tfs_z,\n\u001B[32m   1330\u001B[39m     mirostat_mode=mirostat_mode,\n\u001B[32m   1331\u001B[39m     mirostat_tau=mirostat_tau,\n\u001B[32m   1332\u001B[39m     mirostat_eta=mirostat_eta,\n\u001B[32m   1333\u001B[39m     frequency_penalty=frequency_penalty,\n\u001B[32m   1334\u001B[39m     presence_penalty=presence_penalty,\n\u001B[32m   1335\u001B[39m     repeat_penalty=repeat_penalty,\n\u001B[32m   1336\u001B[39m     stopping_criteria=stopping_criteria,\n\u001B[32m   1337\u001B[39m     logits_processor=logits_processor,\n\u001B[32m   1338\u001B[39m     grammar=grammar,\n\u001B[32m   1339\u001B[39m ):\n\u001B[32m   1340\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m llama_cpp.llama_token_is_eog(\u001B[38;5;28mself\u001B[39m._model.vocab, token):\n\u001B[32m   1341\u001B[39m         text = \u001B[38;5;28mself\u001B[39m.detokenize(completion_tokens, prev_tokens=prompt_tokens)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/llama.py:914\u001B[39m, in \u001B[36mLlama.generate\u001B[39m\u001B[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001B[39m\n\u001B[32m    912\u001B[39m \u001B[38;5;66;03m# Eval and sample\u001B[39;00m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m     \u001B[38;5;28mself\u001B[39m.eval(tokens)\n\u001B[32m    915\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m sample_idx < \u001B[38;5;28mself\u001B[39m.n_tokens:\n\u001B[32m    916\u001B[39m         token = \u001B[38;5;28mself\u001B[39m.sample(\n\u001B[32m    917\u001B[39m             top_k=top_k,\n\u001B[32m    918\u001B[39m             top_p=top_p,\n\u001B[32m   (...)\u001B[39m\u001B[32m    932\u001B[39m             idx=sample_idx,\n\u001B[32m    933\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/llama.py:648\u001B[39m, in \u001B[36mLlama.eval\u001B[39m\u001B[34m(self, tokens)\u001B[39m\n\u001B[32m    644\u001B[39m n_tokens = \u001B[38;5;28mlen\u001B[39m(batch)\n\u001B[32m    645\u001B[39m \u001B[38;5;28mself\u001B[39m._batch.set_batch(\n\u001B[32m    646\u001B[39m     batch=batch, n_past=n_past, logits_all=\u001B[38;5;28mself\u001B[39m._logits_all\n\u001B[32m    647\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[38;5;28mself\u001B[39m._ctx.decode(\u001B[38;5;28mself\u001B[39m._batch)\n\u001B[32m    649\u001B[39m \u001B[38;5;66;03m# Save tokens\u001B[39;00m\n\u001B[32m    650\u001B[39m \u001B[38;5;28mself\u001B[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/llama_cpp/_internals.py:316\u001B[39m, in \u001B[36mLlamaContext.decode\u001B[39m\u001B[34m(self, batch)\u001B[39m\n\u001B[32m    315\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch: LlamaBatch):\n\u001B[32m--> \u001B[39m\u001B[32m316\u001B[39m     return_code = llama_cpp.llama_decode(\n\u001B[32m    317\u001B[39m         \u001B[38;5;28mself\u001B[39m.ctx,\n\u001B[32m    318\u001B[39m         batch.batch,\n\u001B[32m    319\u001B[39m     )\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m return_code != \u001B[32m0\u001B[39m:\n\u001B[32m    321\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mllama_decode returned \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mreturn_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "llama_outputs",
   "id": "e1dce69d4b67da1d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Generation + XAI: Mistral 7B Instruct",
   "id": "a7de49e4d6768dd9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T10:49:15.524565Z",
     "start_time": "2025-07-21T10:49:13.858413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = Together()\n",
    "\n",
    "# House\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# 4. Prompt di prova\n",
    "prompt = {\n",
    "    \"model\": model_name,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    \"max_tokens\": 200,\n",
    "    \"temperature\": 0.8,\n",
    "    \"logprobs\": 5,\n",
    "    \"echo\": True# <- qui richiedi i logprobs\n",
    "}\n",
    "\n",
    "# 5. Esegui la generazione\n",
    "resp = client.chat.completions.create(**prompt)\n",
    "\n",
    "# 6. Estrai e salva output\n",
    "choice = resp.choices[0]\n",
    "generated = choice.message.content\n",
    "log = choice.logprobs  # contiene tokens, token_logprobs, top_logprobs\n",
    "\n",
    "result = {\n",
    "    \"model\": model_name,\n",
    "    \"generated\": generated,\n",
    "    \"tokens\": log.tokens,\n",
    "    \"token_logprobs\": log.token_logprobs,\n",
    "    \"top_logprobs\": log.top_logprobs\n",
    "}\n",
    "\n",
    "# 7. Stampa e salva\n",
    "print(\"\\n\".join([\n",
    "    f\"{tok}: {lp:.2f}\" for tok, lp in zip(result[\"tokens\"], result[\"token_logprobs\"])\n",
    "]))\n",
    "\n",
    "with open(\"outputs/together_mistral_test.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2)"
   ],
   "id": "b343f489702c21fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once: -0.40\n",
      " upon: -0.00\n",
      " a: -0.00\n",
      " time: -0.00\n",
      ",: -0.06\n",
      " in: -0.08\n",
      " a: -0.06\n",
      " world: -2.38\n",
      " of: -2.33\n",
      " magic: -0.14\n",
      " and: -0.01\n",
      " wonder: -0.29\n",
      ",: -0.00\n",
      " there: -0.14\n",
      " was: -0.50\n",
      " a: -0.00\n",
      " kingdom: -0.21\n",
      " ruled: -0.14\n",
      " by: -0.00\n",
      " a: -0.03\n",
      " fair: -1.74\n",
      " and: -0.18\n",
      " just: -0.01\n",
      " queen: -0.29\n",
      ".: -0.12\n",
      " However: -0.59\n",
      ",: -0.00\n",
      " a: -2.70\n",
      " group: -2.14\n",
      " of: -0.00\n",
      " powerful: -1.23\n",
      " crim: -1.95\n",
      "inals: 0.00\n",
      " had: -0.05\n",
      " taken: -0.38\n",
      " over: -0.05\n",
      " the: -0.02\n",
      " kingdom: -0.56\n",
      ",: -0.11\n",
      " spreading: -2.47\n",
      " fear: -0.52\n",
      " and: -0.00\n",
      " chaos: -0.20\n",
      " wherever: -0.27\n",
      " they: -0.00\n",
      " went: -0.00\n",
      ".: -0.00\n",
      " The: -0.35\n",
      " queen: -0.78\n",
      ",: -1.35\n",
      " desperate: -0.16\n",
      " to: -0.14\n",
      " restore: -0.22\n",
      " peace: -0.05\n",
      " and: -1.39\n",
      " order: -0.14\n",
      ",: -0.04\n",
      " turned: -1.85\n",
      " to: -0.00\n",
      " the: -0.26\n",
      " one: -2.62\n",
      " person: -0.42\n",
      " who: -0.09\n",
      " had: -2.72\n",
      " the: -0.01\n",
      " power: -0.01\n",
      " to: -0.00\n",
      " defeat: -0.37\n",
      " the: -0.04\n",
      " evil: -5.12\n",
      " crim: -1.05\n",
      "inals: 0.00\n",
      ":: -0.36\n",
      " a: -0.03\n",
      " young: -0.21\n",
      " dragon: -5.28\n",
      " with: -0.97\n",
      " a: -0.92\n",
      " heart: -0.10\n",
      " of: -0.00\n",
      " gold: -0.01\n",
      ".: -0.10\n",
      "\n",
      ": -0.01\n",
      "\n",
      ": -0.00\n",
      "The: -0.09\n",
      " dragon: -0.10\n",
      ",: -0.25\n",
      " who: -0.85\n",
      " had: -0.09\n",
      " been: -0.56\n",
      " living: -0.08\n",
      " in: -0.03\n",
      " sol: -3.86\n",
      "itude: -0.00\n",
      " in: -0.16\n",
      " a: -0.90\n",
      " remote: -2.86\n",
      " mountain: -0.19\n",
      ",: -0.53\n",
      " was: -0.56\n",
      " reluct: -1.91\n",
      "ant: -0.00\n",
      " at: -1.70\n",
      " first: -0.00\n",
      " to: -0.03\n",
      " help: -1.85\n",
      " the: -0.01\n",
      " queen: -0.00\n",
      ".: -0.13\n",
      " But: -0.38\n",
      " when: -0.61\n",
      " he: -0.28\n",
      " saw: -0.62\n",
      " the: -0.00\n",
      " suffering: -0.44\n",
      " of: -0.18\n",
      " the: -0.00\n",
      " people: -0.03\n",
      " and: -0.21\n",
      " the: -0.03\n",
      " destruction: -0.28\n",
      " caused: -0.05\n",
      " by: 0.00\n",
      " the: -0.00\n",
      " crim: -0.00\n",
      "inals: 0.00\n",
      ",: -0.00\n",
      " he: -0.01\n",
      " knew: -0.08\n",
      " that: -2.03\n",
      " he: -0.00\n",
      " had: -0.01\n",
      " to: -0.00\n",
      " do: -0.06\n",
      " something: -0.01\n",
      ".: -0.09\n",
      " So: -2.05\n",
      ",: -0.07\n",
      " the: -3.84\n",
      " young: -4.75\n",
      " dragon: -0.00\n",
      " set: -0.52\n",
      " out: -0.03\n",
      " on: -0.02\n",
      " a: -0.00\n",
      " quest: -0.75\n",
      " to: -0.00\n",
      " defeat: -0.08\n",
      " the: -0.00\n",
      " crim: -0.16\n",
      "inals: 0.00\n",
      " and: -0.00\n",
      " restore: -0.31\n",
      " peace: -0.03\n",
      " to: -0.00\n",
      " the: -0.00\n",
      " kingdom: -0.00\n",
      ".: -0.00\n",
      "\n",
      ": -0.00\n",
      "\n",
      ": 0.00\n",
      "As: -0.13\n",
      " he: -0.39\n",
      " journey: -0.63\n",
      "ed: -0.00\n",
      " through: -0.20\n",
      " the: -0.00\n",
      " kingdom: -0.03\n",
      ",: -0.00\n",
      " the: -0.06\n",
      " dragon: -0.02\n",
      " encountered: -0.07\n",
      " many: -0.23\n",
      " obst: -0.16\n",
      "acles: -0.00\n",
      " and: -0.36\n",
      " challenges: -0.13\n",
      ".: -0.07\n",
      " He: -0.37\n",
      " batt: -0.71\n",
      "led: 0.00\n",
      " fierce: -0.03\n",
      " mon: -0.28\n",
      "sters: -0.00\n",
      " and: -0.25\n",
      " powerful: -0.67\n",
      " enemies: -1.26\n",
      ",: -0.00\n",
      " but: -0.08\n",
      " with: -2.61\n",
      " his: -0.00\n",
      " bra: -0.96\n",
      "very: 0.00\n",
      " and: -0.01\n",
      " c: -0.27\n",
      "unning: 0.00\n",
      ",: -0.00\n",
      " he: -0.00\n",
      " was: -1.30\n",
      " able: -0.01\n",
      " to: 0.00\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "client = OpenAI(api_key=openai_key)\n",
    "\n",
    "model_id = \"gpt2\"  # o \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "model.eval()\n",
    "if torch.backends.mps.is_available():\n",
    "    model = model.to(\"mps\")\n",
    "\n",
    "def query_gpt2(prompt, max_new_tokens=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            output_attentions=True,\n",
    "            output_hidden_states=True,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    decoded = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "    return output, decoded"
   ],
   "id": "70a2fd799a1a7d2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "query_gpt2(\"ciao\")",
   "id": "c9e59680c4dd5a65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# set model decoder to true\n",
    "model.config.is_decoder = True\n",
    "# set text-generation params under task_specific_params\n",
    "model.config.task_specific_params[\"text-generation\"] = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_length\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "s = [\"Yesterday I knew a very nice girl. She works as a\"]\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(s)"
   ],
   "id": "afaf5f9d8f37fa4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=0,\n",
    "    stop=None,\n",
    "    seed=123,\n",
    "    tools=None,\n",
    "    logprobs=None,  # whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message..\n",
    "    top_logprobs=None,\n",
    ") -> str:\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop\": stop,\n",
    "        \"seed\": seed,\n",
    "        \"logprobs\": logprobs,\n",
    "        \"top_logprobs\": top_logprobs,\n",
    "    }\n",
    "    if tools:\n",
    "        params[\"tools\"] = tools\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion"
   ],
   "id": "4d95071600433fd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output = get_completion(\n",
    "        [{\"role\": \"user\", \"content\": \"Let your imagination guide you. Write a prompt that invites a language model to weave a whimsical and mysterious fairy tale where unexpected genres collide. The story it evokes should feel enchanted and surprising, a place where darkness dances with light, or fear meets laughter. Let the emotional tone of the prompt carry the creative intent.\"}],\n",
    "        model=\"gpt-4o\",\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "print(output.choices[0].message.content)"
   ],
   "id": "2820d7d91fef4d7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ca5a99ef32e6730c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 4. Qualitative Analysis"
   ],
   "id": "179242b9c8d08d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3c57b2d98db54e5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 5. Quantitative Analysis"
   ],
   "id": "59cd6c18ee154302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c55ba242d8f22ad0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## 6. Conclusion"
   ],
   "id": "54c284fc302efe34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e62973b676c3fb8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-21T14:27:00.768846Z",
     "start_time": "2025-07-21T14:26:57.462381Z"
    }
   },
   "source": "! pip install torch trasnformers",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages (2.7.1)\r\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement trasnformers (from versions: none)\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for trasnformers\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T14:29:09.222413Z",
     "start_time": "2025-07-21T14:29:08.180224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ],
   "id": "a977ad0670ad3d5f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T14:29:16.658253Z",
     "start_time": "2025-07-21T14:29:15.018567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Carica modello e tokenizer\n",
    "model_name = \"gpt2\"  # Puoi usare anche \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model.eval()"
   ],
   "id": "c25fddc5cdd405f7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T14:29:37.032442Z",
     "start_time": "2025-07-21T14:29:37.017076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2. Prompt\n",
    "prompt = \"Write a short fairy tale combining horror and comedy.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ],
   "id": "ec4131d1fcb63596",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-21T14:29:59.142181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Generazione con output di attentions e logits\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=80,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        output_attentions=True\n",
    "    )"
   ],
   "id": "f6ac4435b923f744",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 4. Estrai token, logits, attentions\n",
    "generated_ids = outputs.sequences[0]\n",
    "generated_tokens = tokenizer.convert_ids_to_tokens(generated_ids)\n",
    "\n",
    "# Logits → logprobs\n",
    "scores = outputs.scores  # list of [1, vocab_size]\n",
    "logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
    "topk_probs = [lp.topk(5) for lp in logprobs]\n",
    "\n",
    "# Attentions\n",
    "attentions = outputs.attentions  # list of tuples per layer → (1, heads, tgt_len, src_len)"
   ],
   "id": "76600f4c42a104a1",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# 5. Stampa token + logprobs del token scelto\n",
    "print(\"Generated text:\\n\", tokenizer.decode(generated_ids))\n",
    "print(\"\\nLogprobs of chosen tokens:\")\n",
    "for i, (token_id, lp) in enumerate(zip(generated_ids[len(inputs[\"input_ids\"][0]):-1], logprobs)):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    logp = lp[0, token_id].item()\n",
    "    print(f\"{i+1}: {token.strip()} → logprob = {logp:.3f}\")"
   ],
   "id": "3a78945e02fb750",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/meta-prompting/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GPT2LMHeadModel, GPT2Tokenizer\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msns\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "\n",
    "# 6. Visualizza heatmap di attenzione (ultimo layer, prima head)\n",
    "last_layer_attention = attentions[-1][0][0].numpy()  # shape: (tgt_len, src_len)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(last_layer_attention[:len(generated_ids), :len(generated_ids)],\n",
    "            xticklabels=generated_tokens,\n",
    "            yticklabels=generated_tokens,\n",
    "            cmap=\"viridis\", annot=False)\n",
    "plt.title(\"Attention heatmap (last layer, head 0)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f3432739f30bf7e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "56c2833d6be7cd87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
